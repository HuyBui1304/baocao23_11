# -*- coding: utf-8 -*-
"""baocao23-11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lPHqKVqcH4-3u3tlAzjy_01EipX3vNVQ

# DÙNG PHƯƠNG PHÁP TÌM ĐIỂM GẤP KHÚC (KNEE) TRÊN BIỂU ĐỒ SCREE PLOT ĐỂ XÁC ĐỊNH CHỌN SỐ LƯỢNG THÀNH PHẦN CHÍNH TRONG PCA MÀ VẪN GIỮ LẠI ĐƯỢC ĐỘ CHÍNH XÁC MONG MUỐN CỦA DỮ LIỆU
"""

# đọc dữ liệu
import pandas as pd
data = pd.read_csv('/content/drive/MyDrive/AirQualityUCI.csv')

data.head()

import pandas as pd
import numpy as np

# 1. Tổng quan về dữ liệu
print("Tổng quan về dữ liệu:")
print(f"Số dòng (rows): {data.shape[0]}")
print(f"Số cột (columns): {data.shape[1]}\n")

print("Tên các cột và kiểu dữ liệu:")
print(data.dtypes)

print("\nSố lượng giá trị trong mỗi c:")
summary = pd.DataFrame({
    'Total Rows': data.shape[0],
    'Non-Null Values': data.count(),
    'Null Values': data.isnull().sum(),
    'Error Values (-200)': (data == -200).sum(),
    'Unique Values': data.nunique(),
})
print(summary)

print("\nMô tả thống kê cơ bản:")
print(data.describe(include='all'))

data.head()

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler

# Loại bỏ cột không phải dạng số
data = data.select_dtypes(include=[np.number])

# Thay thế giá trị -200 bằng NaN
data = data.replace(-200, np.nan)

# Loại bỏ cột và các dòng có quá nhiều giá trị bị thiếu (loại bỏ các cột có trên 50% và các hàng có trên 30% giá trị bị thiếu)
data = data.loc[:, data.isnull().mean() < 0.5]
data = data[data.isnull().mean(axis=1) < 0.3]

# Điền giá trị thiếu bằng KNN Imputer
imputer = KNNImputer(n_neighbors=5)
data_cleaned = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)

data_cleaned.shape

data_cleaned.head()

"""xem dữ liệu có phân phối chuẩn hay không (Kolmogorov-Smirnov)"""

from scipy.stats import kstest
import pandas as pd
ks_results = []

for column in data_cleaned.columns:
    data = data_cleaned[column].dropna()
    standardized_data = (data - data.mean()) / data.std()
    stat, p_value = kstest(standardized_data, 'norm')
    ks_results.append({
        "Column": column,
        "Statistic": stat,
        "p-value": p_value,
        "Normal Distribution": "Yes" if p_value > 0.05 else "No"
    })

ks_df = pd.DataFrame(ks_results)
print("Kolmogorov-Smirnov Test Results:")
print(ks_df)

import matplotlib.pyplot as plt
import seaborn as sns

num_features = data_cleaned.shape[1]
num_cols = 4
num_rows = (num_features + num_cols - 1) // num_cols
fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, num_rows * 5))
for ax, column in zip(axes.flat, data_cleaned.columns):
    sns.histplot(data_cleaned[column], bins=30, color="blue", ax=ax)
    ax.set_title(f"Histogram của {column}")
    ax.set_xlabel(column)
    ax.set_ylabel("Tần suất")
plt.tight_layout()
plt.show()

"""# xử lý dữ liệu outliers

"""

import matplotlib.pyplot as plt

plt.figure(figsize=(16, 8))
plt.boxplot(data_cleaned, labels=data_cleaned.columns, vert=True, patch_artist=True, showmeans=True)

plt.title("Boxplot")
plt.xlabel("Các đặc trưng (Features)")
plt.ylabel("Giá trị")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

Q1 = data_cleaned.quantile(0.25)
Q3 = data_cleaned.quantile(0.75)
IQR = Q3 - Q1

benduoi = Q1 - 1.5 * IQR
bentren = Q3 + 1.5 * IQR

outliers = (data_cleaned < benduoi) | (data_cleaned > bentren)

data_no_outliers = data_cleaned[~outliers.any(axis=1)]

num_outliers = outliers.sum().sum()
percent_outliers = (num_outliers / data_cleaned.size) * 100
print(f"Số lượng outliers: {num_outliers}, chiếm {percent_outliers:.2f}% dữ liệu.")

for column in data_cleaned.columns:
    column_outliers = data_cleaned[column][outliers[column]]
    print(f"Outliers in column '{column}':")
    print(column_outliers)
    print("-" * 50)

import matplotlib.pyplot as plt

plt.figure(figsize=(16, 8))
plt.boxplot(data_no_outliers, labels=data_no_outliers.columns, vert=True, patch_artist=True, showmeans=True)

plt.title("Dữ liệu sau khi loại bỏ outliers")
plt.xlabel("Các đặc trưng (Features)")
plt.ylabel("Giá trị")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

num_features = data_cleaned.shape[1]
num_cols = 4
num_rows = (num_features + num_cols - 1) // num_cols

fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, num_rows * 5))

for ax, column in zip(axes.flat, data_cleaned.columns):
    sns.histplot(data_cleaned[column], bins=30, label="Dữ liệu ban đầu", color="blue", alpha=0.6, ax=ax)
    sns.histplot(data_no_outliers[column], bins=30, label="Dữ liệu sau khi bỏ outliers", color="red", alpha=0.6, ax=ax)
    ax.axvline(x=benduoi[column], color='red', linestyle='--', label='lower bound')
    ax.axvline(x=bentren[column], color='black', linestyle='--', label='upper bound')
    ax.set_title(f"Histogram của {column}")
    ax.set_xlabel(column)
    ax.set_ylabel("Tần suất")
    ax.legend()
plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 8))
sns.heatmap(data_no_outliers.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Ma trận tương quan giữa các đặc trưng")
plt.show()

### chuẩn hoá dữ liệu
from sklearn.preprocessing import StandardScaler
data_scaled = StandardScaler().fit_transform(data_no_outliers)

data_scaled.shape

from sklearn.decomposition import PCA
pca = PCA()
pca.fit(data_scaled)
# tính tỷ lệ phương sai mà mỗi thành phần chính giải thích và phương sai tích luỹ
explained_variance = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance)

with np.printoptions(precision=2, suppress=True):
    explained_variance_percent = explained_variance * 100
    cumulative_variance_percent = cumulative_variance * 100
    print('tỷ lệ phương sai')
    print(explained_variance_percent)
    print('tỷ lệ phương sai tích luỹ')
    print(cumulative_variance_percent)

"""**tỷ lệ phương sai** : là tỷ lệ mà thành phần chính đó giải thích được bao nhiêu % độ chính xác của dữ liệu góc.

**vd** : thành phần chính thứ nhất là 56,11 => thành phần chính thứ nhất giải thích được 56,11% độ chính xác của dữ liệu gốc.

**phương sai tích luỹ** : phương sai tích luỹ cho ta biết khi lấy số lượng thành phần chính thì giải thích được bao nhiêu % độ chính xác của dữ liệu gốc.

**vd** : khi lấy 2 thành phần chính với thành phần chính thứ 1 giải thích được 56,11% và thành phần chính thứ 2 giải thích được 21,27% thì ta có thể giải thích được độ chính xác của dữ liệu gốc là 56,11% + 21,27% = 77,38%

"""

import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(16, 8))
plt.plot(np.arange(1, len(explained_variance_percent) + 1), explained_variance_percent, marker='o', linestyle='-', label='Explained Variance (%)')
plt.plot(np.arange(1, len(cumulative_variance_percent) + 1), cumulative_variance_percent, marker='o', linestyle='-', label='Cumulative Variance (%)')

threshold = 85
plt.axhline(y=threshold, color='blue', linestyle='--', label=f'Threshold ({threshold}%)')
plt.annotate(f"{threshold}%", xy=(5, threshold), xytext=(4, threshold + 2),
             arrowprops=dict(facecolor='blue', arrowstyle='->'), fontsize=10, color='blue')


component_to_mark = 3
plt.axvline(x = component_to_mark, color='red', linestyle='--', label=f'Component {component_to_mark}')
plt.scatter(component_to_mark, explained_variance_percent[component_to_mark - 1], color='red', s=100)
plt.scatter(component_to_mark, cumulative_variance_percent[component_to_mark - 1], color='red', s=100)
# Thêm nhãn và tiêu đề
plt.xlabel('Component Number')
plt.ylabel('Variance Explained (%)')
plt.title('Scree Plot and Cumulative Variance with Knee Points in Percentage')
plt.legend()
plt.grid(True)

# Hiển thị biểu đồ
plt.show()

"""# THỰC HIỆN PCA VỚI 3 THÀNH PHẦN CHÍNH

Sau khi xác định số lượng thành phần chính phù hợp với độ chính xác mong muốn, tiến hành giảm chiều dữ liệu bằng PCA với số thành phần đã chọn.
"""

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Chuẩn hoá dữ liệu
data_scaled = StandardScaler().fit_transform(data_no_outliers)
##########
# Áp dụng PCA với 3 thành phần chính
pca = PCA(n_components=3,random_state=42)
data_pca = pca.fit_transform(data_scaled)

# Tạo DataFrame cho dữ liệu đã chuyển đổi sang không gian thành phần chính
data_pca_df = pd.DataFrame(data_pca, columns=['PC1', 'PC2', 'PC3'])
###########

# Lấy tỷ lệ phương sai giải thích
explained_variance_ratio = pca.explained_variance_ratio_ * 100
for i in range(len(explained_variance_ratio)):
    for value in [explained_variance_ratio[i]]:
        print(f"Tỷ lệ phương sai giải thích của thành phần chính {i + 1}: {value:.2f}%")
# Tạo DataFrame cho ma trận trọng số của các thành phần chính
loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2', 'PC3'], index=data_cleaned.columns)
print("\nMa trận trọng số của các thành phần chính:")
print(loadings)

# Xuất dữ liệu đã chuyển đổi (đã giảm số chiều) ra tệp CSV
data_pca_df.to_csv('data_pca_reduced.csv', index=False)

data_pca_df

"""PC1 (Mức Độ Ô Nhiễm Tổng Thể):

	•	Trọng số cao nhất:
	•	CO(GT) (0.360814),
	•	PT08.S1(CO) (0.353197),
	•	C6H6(GT) (0.369806),
	•	PT08.S2(NMHC) (0.373624),
	•	PT08.S5(O3) (0.351170).
	•	Ý nghĩa:
	•	Thành phần chính PC1 tập trung chủ yếu vào các đặc trưng liên quan đến mức độ ô nhiễm không khí.
	•	Các đặc trưng này đại diện cho các chất khí và cảm biến quan trọng phản ánh chất lượng không khí tổng thể.
-> PC1: Tóm lược mức độ ô nhiễm tổng thể.

PC2 (Yếu Tố Thời Tiết):

	•	Trọng số cao nhất:
	•	T (0.551594),
	•	AH (0.507702),
	•	PT08.S4(NO2) (0.410928).
	•	Ý nghĩa:
	•	PC2 đại diện cho tác động của yếu tố thời tiết (nhiệt độ và độ ẩm tuyệt đối) lên chất lượng không khí.
	•	Trọng số dương cao của T và AH cho thấy nhiệt độ và độ ẩm là những yếu tố chính trong thành phần này.
-> PC2: Thể hiện tác động của yếu tố thời tiết.

PC3 (Độ Ẩm Tuyệt Đối và Tương Đối):

	•	Trọng số cao nhất:
	•	RH (0.788765),
	•	AH (0.388071),
	•	T (-0.281676).
	•	Ý nghĩa:
	•	Thành phần chính PC3 tập trung vào các đặc trưng liên quan đến độ ẩm tương đối và tuyệt đối.
	•	Trọng số cao của RH cho thấy độ ẩm tương đối là yếu tố chính trong thành phần này, bên cạnh mối liên hệ với nhiệt độ.
-> PC3: Phản ánh độ ẩm (tương đối và tuyệt đối) trong không khí.

# SO SÁNH GIỮA DỮ LIỆU TRƯỚC VÀ SAU KHI ĐƯỢC GIẢM CHIỀU

kích thước của dữ liệu trước và sau khi PCA
"""

# Kích thước của bộ dữ liệu trước PCA
print(f"Kích thước của bộ dữ liệu trước PCA: {data_cleaned.shape}")
# Kích thước của bộ dữ liệu sau PCA
print(f"Kích thước của bộ dữ liệu sau PCA: {data_pca_df.shape}")

cumulative_variance = np.cumsum(pca.explained_variance_ratio_) * 100
print(f"Phương sai tích lũy sau 3 thành phần chính: {cumulative_variance[-1]:.2f}%")

"""Sau khi sử dụng PCA để giảm chiều dữ liệu xuống còn 3 nhưng vẫn giữ được 89.22% độ chính xác

=> phương pháp PCA giúp giảm kích thước dữ liệu nhưng vẫn dữ được độ chính xác của phần lớn dữ liệu

Biểu đồ 3D cho thấy dữ liệu sau khi giảm chiều bằng PCA (với ba thành phần chính: PC1, PC2, PC3) có sự phân bố đồng đều và không còn mối tương quan tuyến tính giữa các chiều. Dữ liệu được trải rộng trong không gian mới của các thành phần chính, cho thấy rằng PCA đã giảm chiều mà vẫn giữ lại phần lớn cấu trúc phân tán tổng quát của dữ liệu gốc.

=> áp dụng PCA cho phép ta giảm xuống 2 hoặc 3 chiều, giúp dễ dàng vẽ biểu đồ và phân tích cấu trúc, cụm dữ liệu trong không gian trực quan hơn.
"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(data_pca_df)
labels = kmeans.labels_
fig = plt.figure(figsize=(16, 14))
ax = fig.add_subplot(111, projection='3d')

scatter = ax.scatter(data_pca_df.iloc[:, 0], data_pca_df.iloc[:, 1], data_pca_df.iloc[:, 2],
                     c=labels, cmap='viridis', label='Clustered Data', alpha=0.8)
ax.set_xlabel('PC1')
ax.set_ylabel('PC2')
ax.set_zlabel('PC3')
ax.set_title('Phân cụm dữ liệu sau khi giảm chiều (PCA)')
legend = ax.legend(*scatter.legend_elements(), title="Clusters")
ax.add_artist(legend)

plt.show()